"""
RAG (Retrieval-Augmented Generation) Chatbot using LangChain v1 and Google Gemini
===================================================================================

This chatbot demonstrates how to build a semantic search engine that:
1. Loads and processes PDF documents
2. Creates embeddings and stores them in a vector database
3. Retrieves relevant context based on user queries
4. Generates responses using an LLM (Google Gemini) with the retrieved context
5. Maintains conversation history (last 10 messages)

Perfect for learning the fundamentals of RAG systems!
"""

# ============================================================================
# IMPORTS - All the tools we need for our RAG chatbot
# ============================================================================

import os
import getpass
from typing import List

# LangChain Core Components
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage

# Document Loading and Processing
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Google Gemini Integration
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# Vector Store (we'll use Chroma - it's simple and works great for demos)
from langchain_chroma import Chroma


# ============================================================================
# CONFIGURATION - Set up API keys and parameters
# ============================================================================

def setup_api_key():
    """
    Prompts user for Google API key if not already set in environment.
    
    To get your API key:
    1. Go to https://makersuite.google.com/app/apikey
    2. Create a new API key
    3. Copy and paste it when prompted
    """
    if not os.environ.get("GOOGLE_API_KEY"):
        print("\nüîë Google API Key Setup")
        print("=" * 50)
        print("Get your API key from: https://makersuite.google.com/app/apikey")
        os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google API Key: ")
        print("‚úÖ API Key set successfully!\n")


# ============================================================================
# DOCUMENT PROCESSING - Load, split, and prepare documents
# ============================================================================

def load_and_process_pdf(pdf_path: str) -> List[Document]:
    """
    Loads a PDF and splits it into smaller chunks for better retrieval.
    
    Why split documents?
    - Large documents are hard for LLMs to process
    - Smaller chunks allow more precise retrieval
    - Better semantic matching with user queries
    
    Args:
        pdf_path: Path to the PDF file (local path or URL)
        
    Returns:
        List of Document objects with content and metadata
    """
    print("\nüìÑ Loading PDF document...")
    
    # Step 1: Load the PDF
    # PyPDFLoader reads PDFs and creates one Document per page
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    print(f"   Loaded {len(documents)} pages from PDF")
    
    # Step 2: Split documents into smaller chunks
    # Why these parameters?
    # - chunk_size=1000: Each chunk is ~1000 characters (about 1-2 paragraphs)
    # - chunk_overlap=200: 200 characters overlap between chunks prevents cutting sentences
    # - add_start_index=True: Keeps track of where each chunk came from
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        add_start_index=True,
        separators=["\n\n", "\n", ". ", " ", ""]  # Split by paragraphs first, then sentences
    )
    
    splits = text_splitter.split_documents(documents)
    print(f"   Split into {len(splits)} chunks for better retrieval\n")
    
    return splits


# ============================================================================
# VECTOR STORE - Create embeddings and set up semantic search
# ============================================================================

def create_vector_store(documents: List[Document]) -> Chroma:
    """
    Creates a vector store from documents for semantic search.
    
    What's happening here?
    1. Each document chunk gets converted to a vector (embedding)
    2. These vectors capture the semantic meaning of the text
    3. Similar text will have similar vectors
    4. We can search by meaning, not just keywords!
    
    Args:
        documents: List of document chunks to embed
        
    Returns:
        Chroma vector store ready for searching
    """
    print("üß† Creating embeddings and vector store...")
    print("   (This may take a minute for large documents)")
    
    # Initialize Google's embedding model
    # This converts text into numerical vectors (embeddings)
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001"  # Google's text embedding model
    )
    
    # Create the vector store
    # Chroma stores our embeddings and allows semantic search
    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name="rag_chatbot_collection",
        persist_directory="./chroma_db"  # Saves to disk so we don't recreate each time
    )
    
    print(f"   ‚úÖ Vector store created with {len(documents)} document chunks!\n")
    
    return vector_store


# ============================================================================
# RAG CHAIN - The heart of our chatbot
# ============================================================================

def create_rag_chain(vector_store: Chroma):
    """
    Creates the RAG (Retrieval-Augmented Generation) chain.
    
    The RAG process:
    1. User asks a question
    2. Retrieve relevant documents from vector store
    3. Pass question + context to LLM
    4. LLM generates answer based on the context
    5. Return answer to user
    
    Args:
        vector_store: The vector store containing our documents
        
    Returns:
        A complete RAG chain ready to answer questions
    """
    
    # Step 1: Create a retriever from our vector store
    # k=4 means retrieve the 4 most similar chunks
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}  # Number of relevant chunks to retrieve
    )
    
    # Step 2: Initialize the Google Gemini chat model
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=0.3,  # Lower = more focused, Higher = more creative
        max_tokens=None,  # Let the model decide response length
        timeout=None,
        max_retries=2,
    )
    
    # Step 3: Create the prompt template
    # This structures how we present the context and question to the LLM
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful assistant answering questions based on the provided context.
        
Instructions:
- Use the context below to answer the user's question
- If the answer isn't in the context, say "I don't have enough information to answer that question."
- Be concise but comprehensive
- Cite specific parts of the context when relevant
- Maintain conversation continuity by considering chat history

Context:
{context}"""),
        MessagesPlaceholder(variable_name="chat_history"),  # Insert conversation history here
        ("human", "{question}")
    ])
    
    # Step 4: Helper function to format retrieved documents
    def format_docs(docs: List[Document]) -> str:
        """Formats retrieved documents into a single string for the prompt"""
        return "\n\n".join(doc.page_content for doc in docs)
    
    # Step 5: Helper function to extract just the question for retrieval
    def extract_question(input_dict):
        """Extracts the question string from the input dictionary"""
        return input_dict["question"]
    
    # Step 6: Build the complete RAG chain
    # This chain: retrieves docs ‚Üí formats them ‚Üí generates response
    rag_chain = (
        {
            "context": extract_question | retriever | format_docs,  # Extract question, retrieve and format documents
            "question": lambda x: x["question"],   # Extract the question
            "chat_history": lambda x: x["chat_history"]  # Extract chat history
        }
        | prompt           # Format into prompt
        | llm              # Send to LLM
        | StrOutputParser()  # Parse response to string
    )
    
    return rag_chain


# ============================================================================
# CONVERSATION MEMORY - Keep track of chat history
# ============================================================================

class ConversationMemory:
    """
    Manages conversation history for the chatbot.
    
    Why limit to 10 messages?
    - Keeps the context window manageable
    - Prevents the LLM from getting overwhelmed
    - Most recent context is usually most relevant
    """
    
    def __init__(self, max_messages: int = 10):
        """
        Args:
            max_messages: Maximum number of message pairs to keep (default: 10)
        """
        self.chat_history: List = []
        self.max_messages = max_messages
    
    def add_message(self, human_message: str, ai_message: str):
        """
        Adds a question-answer pair to history.
        
        Args:
            human_message: The user's question
            ai_message: The AI's response
        """
        self.chat_history.append(HumanMessage(content=human_message))
        self.chat_history.append(AIMessage(content=ai_message))
        
        # Keep only the last N messages (2 messages = 1 Q&A pair)
        max_history_length = self.max_messages * 2
        if len(self.chat_history) > max_history_length:
            self.chat_history = self.chat_history[-max_history_length:]
    
    def get_history(self):
        """Returns the current chat history"""
        return self.chat_history
    
    def clear(self):
        """Clears the conversation history"""
        self.chat_history = []


# ============================================================================
# CLI INTERFACE - User interaction
# ============================================================================

def run_chatbot(rag_chain, memory: ConversationMemory):
    """
    Runs the interactive CLI chatbot interface.
    
    Commands:
    - Type your question and press Enter
    - Type 'clear' to reset conversation history
    - Type 'quit' or 'exit' to end the session
    
    Args:
        rag_chain: The RAG chain for generating responses
        memory: Conversation memory manager
    """
    print("\n" + "=" * 70)
    print("ü§ñ RAG CHATBOT - Ready to answer your questions!")
    print("=" * 70)
    print("\nCommands:")
    print("  ‚Ä¢ Type your question and press Enter")
    print("  ‚Ä¢ Type 'clear' to reset conversation history")
    print("  ‚Ä¢ Type 'quit' or 'exit' to end the session\n")
    print("=" * 70 + "\n")
    
    while True:
        # Get user input
        user_input = input("üë§ You: ").strip()
        
        # Handle special commands
        if user_input.lower() in ['quit', 'exit']:
            print("\nüëã Thanks for chatting! Goodbye!\n")
            break
        
        if user_input.lower() == 'clear':
            memory.clear()
            print("\nüóëÔ∏è  Conversation history cleared!\n")
            continue
        
        if not user_input:
            print("‚ö†Ô∏è  Please enter a question.\n")
            continue
        
        try:
            # Generate response using the RAG chain
            print("\nü§ñ Assistant: ", end="", flush=True)
            
            # Invoke the chain with the question and chat history
            response = rag_chain.invoke({
                "question": user_input,
                "chat_history": memory.get_history()
            })
            
            print(response + "\n")
            
            # Add to conversation history
            memory.add_message(user_input, response)
            
        except Exception as e:
            print(f"\n‚ùå Error: {str(e)}\n")
            print("Please try again or check your API key.\n")


# ============================================================================
# MAIN - Put it all together
# ============================================================================

def main():
    """
    Main function that orchestrates the entire RAG chatbot setup and execution.
    """
    
    print("\n" + "=" * 70)
    print("üöÄ INITIALIZING RAG CHATBOT")
    print("=" * 70)
    
    # Step 1: Setup API key
    setup_api_key()
    
    # Step 2: Load and process the PDF
    # Using a sample PDF - you can replace this with any PDF URL or local path
    # Example: A research paper on AI that's publicly available
    pdf_path = "https://arxiv.org/pdf/1706.03762.pdf"  # "Attention Is All You Need" paper
    print(f"üì• Using PDF: {pdf_path}")
    print("   (You can change this to any PDF path in the code)\n")
    
    documents = load_and_process_pdf(pdf_path)
    
    # Step 3: Create vector store
    vector_store = create_vector_store(documents)
    
    # Step 4: Create RAG chain
    print("‚öôÔ∏è  Creating RAG chain...")
    rag_chain = create_rag_chain(vector_store)
    print("   ‚úÖ RAG chain ready!\n")
    
    # Step 5: Initialize conversation memory
    memory = ConversationMemory(max_messages=10)
    
    # Step 6: Run the chatbot
    run_chatbot(rag_chain, memory)


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    """
    This is where the program starts!
    
    To run this chatbot:
    1. Install required packages:
       pip install langchain langchain-google-genai langchain-community langchain-chroma pypdf chromadb
    
    2. Run the script:
       python rag_chatbot.py
    
    3. Enter your Google API key when prompted
    
    4. Start asking questions about the document!
    
    Example questions to try:
    - "What is this document about?"
    - "Can you summarize the main points?"
    - "What are the key findings?"
    """
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user. Exiting gracefully...\n")
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {str(e)}\n")
        print("Please check your setup and try again.\n")
